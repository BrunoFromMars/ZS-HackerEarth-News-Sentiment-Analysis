# -*- coding: utf-8 -*-
"""ZS_title.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/166YUUk837kBfuI8Ew4KyaM8EhlWX4Cds
"""

import pandas as pd
import numpy as np


import nltk
nltk.download('stopwords')
nltk.download('punkt')


from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.stem import PorterStemmer

ps = PorterStemmer()
#from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer1 = TfidfVectorizer(sublinear_tf=True, max_df=1.0)

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

import re


import datetime

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Activation, Reshape, GlobalAveragePooling1D, Dropout, Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json

train_url = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/train_file_1.csv'

df_new = pd.read_csv(train_url)

appos = {
"aren't" : "are not",
"can't" : "cannot",
"couldn't" : "could not",
"didn't" : "did not",
"doesn't" : "does not",
"don't" : "do not",
"hadn't" : "had not",
"hasn't" : "has not",
"haven't" : "have not",
"he'd" : "he would",
"he'll" : "he will",
"he's" : "he is",
"i'd" : "I would",
"i'd" : "I had",
"i'll" : "I will",
"i'm" : "I am",
"isn't" : "is not",
"it's" : "it is",
"it'll":"it will",
"i've" : "I have",
"let's" : "let us",
"mightn't" : "might not",
"mustn't" : "must not",
"shan't" : "shall not",
"she'd" : "she would",
"she'll" : "she will",
"she's" : "she is",
"shouldn't" : "should not",
"that's" : "that is",
"there's" : "there is",
"they'd" : "they would",
"they'll" : "they will",
"they're" : "they are",
"they've" : "they have",
"we'd" : "we would",
"we're" : "we are",
"weren't" : "were not",
"we've" : "we have",
"what'll" : "what will",
"what're" : "what are",
"what's" : "what is",
"what've" : "what have",
"where's" : "where is",
"who'd" : "who would",
"who'll" : "who will",
"who're" : "who are",
"who's" : "who is",
"who've" : "who have",
"won't" : "will not",
"wouldn't" : "would not",
"you'd" : "you would",
"you'll" : "you will",
"you're" : "you are",
"you've" : "you have",
"'re": " are",
"wasn't": "was not",
"we'll":" will",
"didn't": "did not"
}

processed_title = []

for i in range(df_new.shape[0]):
    t = df_new.Title[i].lower().split()
    #h = df_new.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    #reformed_h = [appos[word] if word in appos else word for word in h]
    #h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    #h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    #words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    #filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    #h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    #for w in filtered_h:
    #   h += ps.stem(w) + " "
        
    processed_title.append(t)
    #processed_headline.append(h)
#print(t)
#print(h)

bow1 = vectorizer1.fit_transform(processed_title)

freqs1 = [(word, bow1.getcol(idx).sum()) for word, idx in vectorizer1.vocabulary_.items()]
results1=sorted (freqs1, key = lambda x: -x[1])

df_t = svd.fit_transform(bow1) 
#df_p.shape

df_t = pd.DataFrame(df_t)

df_t = min_max_scaler.fit_transform(df_t)

df_t = pd.DataFrame(df_t)

df_final = df_t

url_test = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/test_file_1.csv'

df_test = pd.read_csv(url_test)

list_source = df_new['Source'].tolist()

for i in range(df_test.shape[0]):
    list_source.append(df_test.Source[i])

len(list_source)




df_source = pd.DataFrame(list_source,columns=['Source'])

df_source = pd.get_dummies(df_source)

#df_source.shape

df_final = df_final.join(df_source.iloc[0:df_new.shape[0],:])

df_final.isnull().sum().sum()

df_final = df_final.join(pd.get_dummies(df_new['Topic']))

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[] 


for i in range(df_new.shape[0]):
    string = df_new.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)

df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])


df_final = df_final.join(df_date)

col_sm = ['Facebook', 'GooglePlus', 'LinkedIn']
df_sm = df_new[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final = df_final.join(df_sm)

df_final = df_final.join(df_new[['SentimentTitle']])

df_final.shape

df_final.isnull().sum().sum()

"""# Model Making"""

model_m = Sequential()
model_m.add(Reshape((101, 67), input_shape=(6767,)))
model_m.add(Conv1D(100, 10, activation='relu', input_shape=(101, 67)))
model_m.add(Conv1D(100, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Dropout(0.2))
#model_m.add(Flatten())
model_m.add(Dense(75, activation='relu'))
model_m.add(Flatten())
model_m.add(Dropout(0.25))
model_m.add(Dense(25, input_dim=75, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(10, input_dim=25, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(1, input_dim=10, activation='tanh'))
print(model_m.summary())

model_m.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

model_m.fit(df_final.iloc[:,0:-1],df_final.iloc[:,-1:],batch_size=500,epochs=5,verbose=1,callbacks=None,validation_split=0.2)

!pip install h5py

model_c1d_8April_2259_t_json = model_m.to_json()

with open("model_c1d_8April_2259_t.json", "w") as json_file:
    json_file.write(model_c1d_8April_2259_t_json)

model_m.save_weights("model_c1d_8April_2259_t.h5")
print("Saved model to disk")

processed_title_test = []

for i in range(df_test.shape[0]):
    t = df_test.Title[i].lower().split()
    #h = df_test.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    #reformed_h = [appos[word] if word in appos else word for word in h]
    #h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    #h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    #words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    #filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    #h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    #for w in filtered_h:
    #    h += ps.stem(w) + " "
        
    processed_title_test.append(t)
    #processed_headline_test.append(h)

bow1_test = vectorizer1.fit_transform(processed_title_test)

df_t_test = svd.fit_transform(bow1_test)

df_t_test = pd.DataFrame(df_t_test)

df_t_test = min_max_scaler.fit_transform(df_t_test)

df_t_test = pd.DataFrame(df_t_test)

df_final_test = df_t_test

df_final_test = df_final_test.join(df_source.iloc[df_new.shape[0]:,:])

df_final_test = df_final_test.join( pd.get_dummies(df_test['Topic']))

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[]



for i in range(df_test.shape[0]):
    string = df_test.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)

df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_final_test = df_final_test.join(df_date)

col_sm = ['Facebook', 'GooglePlus', 'LinkedIn']
df_sm = df_test[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final_test = df_final_test.join(df_sm)

df_final_test.head()

#len(df_final_test.columns[df_final_test.isna().any()].tolist())

#df_final_test.isnull().sum().sum()/5754

json_file = open('model_c1d_8April_2259_t.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

loaded_model.load_weights("model_c1d_8April_2259_t.h5")
print("Loaded model from disk")

loaded_model.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

predict_t = loaded_model.predict(df_final_test,verbose=1)

predict_t

pd.DataFrame(predict_t).to_csv('predicted_t.csv')

p_t = pd.DataFrame(predict_t)

p_t.describe()