# -*- coding: utf-8 -*-

"""

Below is the code for SentimentHeadline

"""
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ghT7NND6Rax02pk0FLJODWZZAJpWznZj
"""

import pandas as pd
import numpy as np


import nltk
nltk.download('stopwords')
nltk.download('punkt')


from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.stem import PorterStemmer

ps = PorterStemmer()
#from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

import re


import datetime

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Activation, Reshape, GlobalAveragePooling1D, Dropout, Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json


"""
import pandas as pd



df = pd.read_csv('dataset/train_file.csv')


import csv

with open('dataset/train_file.csv', 'r') as f:
    reader = csv.reader(f)
    your_list = list(reader)





for i in range(len(your_list)):
    for j in range(1,4):
        your_list[i][j] = your_list[i][j].replace('&#39;','\'')
        your_list[i][j] = your_list[i][j].replace('&quot;','\'')
        your_list[i][j] = your_list[i][j].replace(';','.')
    



df_list = pd.DataFrame(your_list[1:]) 
df_list.to_csv('dataset/train_file_1.csv')


df = pd.read_csv('dataset/test_file.csv')



with open('dataset/test_file.csv', 'r') as f:
    reader = csv.reader(f)
    your_list = list(reader)
    
    
    
for i in range(len(your_list)):
    for j in range(1,4):
        your_list[i][j] = your_list[i][j].replace('&#39;','\'')
        your_list[i][j] = your_list[i][j].replace('&quot;','\'')
        your_list[i][j] = your_list[i][j].replace(';','.')
    
df_list = pd.DataFrame(your_list[1:]) 
df_list.to_csv('dataset/test_file_1.csv')



"""






train_url = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/train_file_2.csv'

df_new = pd.read_csv(train_url)

df_new.head()







appos = {
"aren't" : "are not",
"can't" : "cannot",
"couldn't" : "could not",
"didn't" : "did not",
"doesn't" : "does not",
"don't" : "do not",
"hadn't" : "had not",
"hasn't" : "has not",
"haven't" : "have not",
"he'd" : "he would",
"he'll" : "he will",
"he's" : "he is",
"i'd" : "I would",
"i'd" : "I had",
"i'll" : "I will",
"i'm" : "I am",
"isn't" : "is not",
"it's" : "it is",
"it'll":"it will",
"i've" : "I have",
"let's" : "let us",
"mightn't" : "might not",
"mustn't" : "must not",
"shan't" : "shall not",
"she'd" : "she would",
"she'll" : "she will",
"she's" : "she is",
"shouldn't" : "should not",
"that's" : "that is",
"there's" : "there is",
"they'd" : "they would",
"they'll" : "they will",
"they're" : "they are",
"they've" : "they have",
"we'd" : "we would",
"we're" : "we are",
"weren't" : "were not",
"we've" : "we have",
"what'll" : "what will",
"what're" : "what are",
"what's" : "what is",
"what've" : "what have",
"where's" : "where is",
"who'd" : "who would",
"who'll" : "who will",
"who're" : "who are",
"who's" : "who is",
"who've" : "who have",
"won't" : "will not",
"wouldn't" : "would not",
"you'd" : "you would",
"you'll" : "you will",
"you're" : "you are",
"you've" : "you have",
"'re": " are",
"wasn't": "was not",
"we'll":" will",
"didn't": "did not"
}

"""

def normalize_text(text):
    text=text.lower()
    text = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(pic\.twitter\.com/[^\s]+))','', text)
    text = re.sub('@[^\s]+','', text)
    text = re.sub('#([^\s]+)', '', text)
    text = re.sub('[:;>?<=*+()&,\-#!$%\{˜|\}\[^_\\@\]1234567890’‘]',' ', text)
    text = re.sub('[\d]','', text)
    text = text.replace(".", '')
    text = text.replace("'", '')
    text = text.replace("`", '')
    text = text.replace("'s", '')
    text = text.replace("/", ' ')
    text = text.replace("\"", ' ')
    text = text.replace("\\", '')
    #text =  re.sub(r"\b[a-z]\b", "", text)
    text=re.sub( '\s+', ' ', text).strip()
    
    return text
"""



processed_title,processed_headline = [],[]
for i in range(df_new.shape[0]):
    t = df_new.Title[i].lower().split()
    h = df_new.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    reformed_h = [appos[word] if word in appos else word for word in h]
    h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    for w in filtered_h:
        h += ps.stem(w) + " "
        
    processed_title.append(t)
    processed_headline.append(h)
#print(t)
#print(h)


"""

document_t = df_new.Title.tolist()
document_h = df_new.Headline.tolist()


docs_t=[]
for document in document_t:
    text=normalize_text(document)
    nl_text=''
    for word in word_tokenize(text):
        if word not in stopwords:
            nl_text+=(lemmatizer.lemmatize(word))+' '
    docs_t.append(nl_text)



docs_h=[]
for document in document_h:
    text=normalize_text(document)
    nl_text=''
    for word in word_tokenize(text):
        if word not in stopwords:
            nl_text+=(lemmatizer.lemmatize(word))+' '
    docs_h.append(nl_text)

"""



vectorizer1 = TfidfVectorizer(sublinear_tf=True, max_df=1.0)
bow1 = vectorizer1.fit_transform(processed_title)

freqs1 = [(word, bow1.getcol(idx).sum()) for word, idx in vectorizer1.vocabulary_.items()]
results1=sorted (freqs1, key = lambda x: -x[1])
#print(results1)

#bow1.shape

vectorizer2 = TfidfVectorizer(sublinear_tf=True, max_df=1.0)
bow1_h = vectorizer2.fit_transform(processed_headline)

freqs1_h   = [(word, bow1_h.getcol(idx).sum()) for word, idx in vectorizer2.vocabulary_.items()]
results1_h = sorted (freqs1_h, key = lambda x: -x[1])
#feature_names = vectorizer1.get_feature_names()
#corpus_index = [n for n in docs_t]

#df_t = pd.DataFrame(bow1.todense(), index=corpus_index, columns=feature_names)

#df_new = df_new.set_index('IDLink')
#df_t.index = df_new.index






df_p = svd.fit_transform(bow1) 
df_p.shape


#print(results1)

df_h = svd.fit_transform(bow1_h) 

df_t= df_p
df_t[0][0:5]

df_t = pd.DataFrame(df_t)
df_h = pd.DataFrame(df_h)



df_t = min_max_scaler.fit_transform(df_t)
df_h = min_max_scaler.fit_transform(df_h)

df_t = pd.DataFrame(df_t)
df_h = pd.DataFrame(df_h)

df_final = df_t.join(df_h,lsuffix='_t',rsuffix='_h')

df_final.head()



url_test = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/test_file_1.csv'

df_test = pd.read_csv(url_test)

list_source = df_new['Source'].tolist()

for i in range(df_test.shape[0]):
    list_source.append(df_test.Source[i])

len(list_source)




df_source = pd.DataFrame(list_source,columns=['Source'])

df_source = pd.get_dummies(df_source)

#df_source.shape

df_final = df_final.join(df_source.iloc[0:df_new.shape[0],:])

df_final.head()



df_final = df_final.join(pd.get_dummies(df_new['Topic']))

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[] 


for i in range(df_new.shape[0]):
    string = df_new.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)
    

"""
list_year = min_max_scaler.fit_transform(np.array(list_year))
list_month = min_max_scaler.fit_transform(np.array(list_month).reshape(-1,1))
list_day = min_max_scaler.fit_transform(np.array(list_day).reshape(-1,1))
list_hour = min_max_scaler.fit_transform(np.array(list_hour).reshape(-1,1))
list_min = min_max_scaler.fit_transform(np.array(list_min).reshape(-1,1))
list_sec = min_max_scaler.fit_transform(np.array(list_sec).reshape(-1,1))

    """



df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])


df_final = df_final.join(df_date)

df_final.head()




df_sm = df_new[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final = df_final.join(df_sm)

df_final = df_final.join(df_new[['SentimentTitle','SentimentHeadline']])


df_final.head()

df_final.shape

df_final.head()





"""# Model Making Conv1D with Dense"""



model_m = Sequential()
model_m.add(Reshape((863, 9), input_shape=(7767,)))
model_m.add(Conv1D(100, 10, activation='relu', input_shape=(123, 55)))
model_m.add(Conv1D(100, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Dropout(0.2))
#model_m.add(Flatten())
model_m.add(Dense(75, activation='relu'))
model_m.add(Flatten())
model_m.add(Dropout(0.25))
model_m.add(Dense(25, input_dim=75, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(10, input_dim=25, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(1, input_dim=10, activation='tanh'))
print(model_m.summary())


X = df_final.iloc[:,0:-2]

X = X.as_matrix()

Y = df_final.iloc[:,-1:].as_matrix()

#Y = min_max_scaler.fit_transform(Y)

#training_samples = 40000
#validation_samples = 55932-40000

#x_train = X[:training_samples]
#y_train = Y[:training_samples]
#x_val = X[training_samples:training_samples + validation_samples]
#y_val = Y[training_samples:training_samples + validation_samples]

#x_train = []
#x_val = []

model_m.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

#!pip install h5py

model_m.fit(X,Y,batch_size=500,epochs=30,verbose=1,callbacks=None,validation_split=0.2)

#saving model
model_c1d_8April_1820_json = model_m.to_json()

with open("model_c1d_8April_1820.json", "w") as json_file:
    json_file.write(model_c1d_8April_1820_json)

model_m.save_weights("model_c1d_8April_1820.h5")
print("Saved model to disk")
""" Deep Neural Network Simple"""
"""
model_d = Sequential()
model_d.add(Dense(1000, input_dim=6765, activation='relu'))
model_d.add(Dropout(0.5))
model_d.add(Dense(700, input_dim=1000, activation='relu'))
model_d.add(Dropout(0.5))
model_d.add(Dense(500, input_dim=700, activation='relu'))
model_d.add(Dropout(0.2))
model_d.add(Dense(100, input_dim=500, activation='relu'))
model_d.add(Dropout(0.2))
model_d.add(Dense(50, input_dim=100, activation='relu'))
model_d.add(Dropout(0.2))
model_d.add(Dense(10, input_dim=50, activation='relu'))
model_d.add(Dropout(0.2))
model_d.add(Dense(1, input_dim=10, activation='tanh'))
print(model_d.summary())

model_d.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

model_d.fit(x_train,y_train,batch_size=500,epochs=30,verbose=1,callbacks=None,validation_data=(x_val,y_val))

model_d_json = model_d.to_json()
with open("model_d.json", "w") as json_file:
    json_file.write(model_d_json)

model_d.save_weights("model_d.h5")
print("Saved model to disk")

"""
"""Deep Neural Network Simple"""

"""

model_d2 = Sequential()
model_d2.add(Dense(3000, input_dim=6765, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(1000, input_dim=3000, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(500, input_dim=1000, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(100, input_dim=500, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(50, input_dim=100, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(10, input_dim=50, activation='relu'))
model_d2.add(Dropout(0.2))
model_d2.add(Dense(1, input_dim=10, activation='tanh'))
print(model_d2.summary())

model_d2.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

model_d2.fit(x_train,y_train,batch_size=500,epochs=60,verbose=1,callbacks=None,validation_data=(x_val,y_val))

model_d2_json = model_d2.to_json()
with open("model_d2.json", "w") as json_file:
    json_file.write(model_d2_json)

model_d2.save_weights("model_d2.h5")
print("Saved model to disk")
"""

""" Conv2D with Dense """

"""
model_c2 = Sequential()

model_c2.add(Reshape((123, 55,1), input_shape=(6765,)))
model_c2.add(Conv2D(100, kernel_size=(3, 3), activation='relu', input_shape=(123, 55, 1)))
model_c2.add(Conv2D(200,kernel_size=(3, 3), activation='relu'))
model_c2.add(MaxPooling2D(pool_size=(2, 2)))
model_c2.add(Dropout(0.25))
model_c2.add(Flatten())
model_c2.add(Dense(20, activation='relu'))
model_c2.add(Dropout(0.25))
model_c2.add(Dense(15, input_dim=20, activation='relu'))
model_c2.add(Dropout(0.25))
model_c2.add(Dense(10, input_dim=15 , activation='relu'))
model_c2.add(Dropout(0.25))
model_c2.add(Dense(1, input_dim=10 ,activation='tanh'))
print(model_c2.summary())

model_c2.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

filepath="weights-improvement-{epoch:02d}-{val_mae:.2f}.hdf5"


checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')

callbacks_list = [checkpoint]

model_c2.fit(x_train,y_train,batch_size=200,epochs=30,callbacks=callbacks_list, verbose=1,validation_data=(x_val,y_val))

model_c2_json = model_c2.to_json()
with open("model_c2.json", "w") as json_file:
    json_file.write(model_c2_json)

model_c2.save_weights("model_c2.h5")
print("Saved model to disk")

"""
"""# Testing phase"""



processed_title_test,processed_headline_test = [],[]
for i in range(df_test.shape[0]):
    t = df_test.Title[i].lower().split()
    h = df_test.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    reformed_h = [appos[word] if word in appos else word for word in h]
    h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    for w in filtered_h:
        h += ps.stem(w) + " "
        
    processed_title_test.append(t)
    processed_headline_test.append(h)

bow1_test = vectorizer1.fit_transform(processed_title_test)

freqs1 = [(word, bow1_test.getcol(idx).sum()) for word, idx in vectorizer1.vocabulary_.items()]
results1=sorted (freqs1, key = lambda x: -x[1])

df_t_test = svd.fit_transform(bow1_test)

bow1_h_test = vectorizer2.fit_transform(processed_headline)

freqs1_h   = [(word, bow1_h_test.getcol(idx).sum()) for word, idx in vectorizer2.vocabulary_.items()]
results1_h = sorted (freqs1_h, key = lambda x: -x[1])

df_h_test = svd.fit_transform(bow1_h_test)

df_t_test = pd.DataFrame(df_t_test)
df_h_test = pd.DataFrame(df_h_test)

df_t_test = min_max_scaler.fit_transform(df_t_test)
df_h_test = min_max_scaler.fit_transform(df_h_test)

df_t_test = pd.DataFrame(df_t_test)
df_h_test = pd.DataFrame(df_h_test)

df_final_test = df_t_test.join(df_h_test,lsuffix='_t',rsuffix='_h')





df_final_test = df_final_test.join(df_source.iloc[df_new.shape[0]:,:])

one_hot_topic_test  = pd.get_dummies(df_test['Topic'])

df_final_test = df_final_test.join(one_hot_topic_test)

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[]



for i in range(df_test.shape[0]):
    string = df_test.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)

df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_final_test = df_final_test.join(df_date)

col_sm = ['Facebook', 'GooglePlus', 'LinkedIn']
df_sm = df_test[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final_test = df_final_test.join(df_sm)

df_final_test.head()

"""# Prediction"""


json_file = open('model_c1d_8April_1820.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

loaded_model.load_weights("model_c1d_8April_1820.h5")
print("Loaded model from disk")

loaded_model.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])



predict_h = loaded_model.predict(df_final_test,verbose=1)

pd.DataFrame(predict_h).to_csv('predicted_h.csv')

p_h = pd.DataFrame(predict_h)

p_h.describe()

"""

Below is the code for sentimentTitle

"""

"""
# -*- coding: utf-8 -*-
ZS_title.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/166YUUk837kBfuI8Ew4KyaM8EhlWX4Cds


import pandas as pd
import numpy as np


import nltk
nltk.download('stopwords')
nltk.download('punkt')


from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from nltk.stem import PorterStemmer

ps = PorterStemmer()
#from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer1 = TfidfVectorizer(sublinear_tf=True, max_df=1.0)

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000)

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

import re


import datetime

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Activation, Reshape, GlobalAveragePooling1D, Dropout, Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json

train_url = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/train_file_1.csv'

df_new = pd.read_csv(train_url)

appos = {
"aren't" : "are not",
"can't" : "cannot",
"couldn't" : "could not",
"didn't" : "did not",
"doesn't" : "does not",
"don't" : "do not",
"hadn't" : "had not",
"hasn't" : "has not",
"haven't" : "have not",
"he'd" : "he would",
"he'll" : "he will",
"he's" : "he is",
"i'd" : "I would",
"i'd" : "I had",
"i'll" : "I will",
"i'm" : "I am",
"isn't" : "is not",
"it's" : "it is",
"it'll":"it will",
"i've" : "I have",
"let's" : "let us",
"mightn't" : "might not",
"mustn't" : "must not",
"shan't" : "shall not",
"she'd" : "she would",
"she'll" : "she will",
"she's" : "she is",
"shouldn't" : "should not",
"that's" : "that is",
"there's" : "there is",
"they'd" : "they would",
"they'll" : "they will",
"they're" : "they are",
"they've" : "they have",
"we'd" : "we would",
"we're" : "we are",
"weren't" : "were not",
"we've" : "we have",
"what'll" : "what will",
"what're" : "what are",
"what's" : "what is",
"what've" : "what have",
"where's" : "where is",
"who'd" : "who would",
"who'll" : "who will",
"who're" : "who are",
"who's" : "who is",
"who've" : "who have",
"won't" : "will not",
"wouldn't" : "would not",
"you'd" : "you would",
"you'll" : "you will",
"you're" : "you are",
"you've" : "you have",
"'re": " are",
"wasn't": "was not",
"we'll":" will",
"didn't": "did not"
}

processed_title = []

for i in range(df_new.shape[0]):
    t = df_new.Title[i].lower().split()
    #h = df_new.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    #reformed_h = [appos[word] if word in appos else word for word in h]
    #h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    #h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    #words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    #filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    #h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    #for w in filtered_h:
    #   h += ps.stem(w) + " "
        
    processed_title.append(t)
    #processed_headline.append(h)
#print(t)
#print(h)

bow1 = vectorizer1.fit_transform(processed_title)

freqs1 = [(word, bow1.getcol(idx).sum()) for word, idx in vectorizer1.vocabulary_.items()]
results1=sorted (freqs1, key = lambda x: -x[1])

df_t = svd.fit_transform(bow1) 
#df_p.shape

df_t = pd.DataFrame(df_t)

df_t = min_max_scaler.fit_transform(df_t)

df_t = pd.DataFrame(df_t)

df_final = df_t

url_test = 'https://raw.githubusercontent.com/BrunoFromMars/ZS/master/test_file_1.csv'

df_test = pd.read_csv(url_test)

list_source = df_new['Source'].tolist()

for i in range(df_test.shape[0]):
    list_source.append(df_test.Source[i])

len(list_source)




df_source = pd.DataFrame(list_source,columns=['Source'])

df_source = pd.get_dummies(df_source)

#df_source.shape

df_final = df_final.join(df_source.iloc[0:df_new.shape[0],:])

df_final.isnull().sum().sum()

df_final = df_final.join(pd.get_dummies(df_new['Topic']))

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[] 


for i in range(df_new.shape[0]):
    string = df_new.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)

df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])


df_final = df_final.join(df_date)

col_sm = ['Facebook', 'GooglePlus', 'LinkedIn']
df_sm = df_new[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final = df_final.join(df_sm)

df_final = df_final.join(df_new[['SentimentTitle']])

df_final.shape

df_final.isnull().sum().sum()

# Model Making

model_m = Sequential()
model_m.add(Reshape((101, 67), input_shape=(6767,)))
model_m.add(Conv1D(100, 10, activation='relu', input_shape=(101, 67)))
model_m.add(Conv1D(100, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(Conv1D(200, 10, activation='relu'))
model_m.add(MaxPooling1D(3))
model_m.add(Dropout(0.2))
#model_m.add(Flatten())
model_m.add(Dense(75, activation='relu'))
model_m.add(Flatten())
model_m.add(Dropout(0.25))
model_m.add(Dense(25, input_dim=75, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(10, input_dim=25, activation='relu'))
model_m.add(Dropout(0.25))
model_m.add(Dense(1, input_dim=10, activation='tanh'))
print(model_m.summary())

model_m.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

model_m.fit(df_final.iloc[:,0:-1],df_final.iloc[:,-1:],batch_size=500,epochs=5,verbose=1,callbacks=None,validation_split=0.2)

!pip install h5py

model_c1d_8April_2259_t_json = model_m.to_json()

with open("model_c1d_8April_2259_t.json", "w") as json_file:
    json_file.write(model_c1d_8April_2259_t_json)

model_m.save_weights("model_c1d_8April_2259_t.h5")
print("Saved model to disk")

processed_title_test = []

for i in range(df_test.shape[0]):
    t = df_test.Title[i].lower().split()
    #h = df_test.Headline[i].lower().split()
    #print(t)
    #print(h)
    #negation handling
    reformed_t = [appos[word] if word in appos else word for word in t]
    t = " ".join(reformed_t) 
    #reformed_h = [appos[word] if word in appos else word for word in h]
    #h = " ".join(reformed_h)
    #tokenize
    t = word_tokenize(t)
    #h = word_tokenize(h)
    #alpha numeric
    words_t = [word for word in t if word.isalpha()]
    #words_h = [word for word in h if word.isalpha()]
    #stop-words
    filtered_t = [w for w in words_t if not w in stop_words]
    #filtered_h = [w for w in words_h if not w in stop_words]
    #stemming
    t =""
    #h =""
    for w in filtered_t:
        t += ps.stem(w) + " "
    #for w in filtered_h:
    #    h += ps.stem(w) + " "
        
    processed_title_test.append(t)
    #processed_headline_test.append(h)

bow1_test = vectorizer1.fit_transform(processed_title_test)

df_t_test = svd.fit_transform(bow1_test)

df_t_test = pd.DataFrame(df_t_test)

df_t_test = min_max_scaler.fit_transform(df_t_test)

df_t_test = pd.DataFrame(df_t_test)

df_final_test = df_t_test

df_final_test = df_final_test.join(df_source.iloc[df_new.shape[0]:,:])

df_final_test = df_final_test.join( pd.get_dummies(df_test['Topic']))

list_year, list_month, list_day, list_hour, list_min, list_sec = [],[],[],[],[],[]



for i in range(df_test.shape[0]):
    string = df_test.PublishDate[i]
    date = datetime.datetime.strptime(string, "%Y-%m-%d %H:%M:%S") 
    list_year.append(date.year)
    list_month.append(date.month)
    list_day.append(date.day)
    list_hour.append(date.hour)
    list_min.append(date.minute)
    list_sec.append(date.second)

df_date = pd.DataFrame(list(zip(list_year, list_month, list_day, list_hour, list_min, list_sec)), 
               columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_date = min_max_scaler.fit_transform(df_date)

df_date = pd.DataFrame(df_date,columns =['list_year', 'list_month', 'list_day', 'list_hour', 'list_min', 'list_sec'])

df_final_test = df_final_test.join(df_date)

col_sm = ['Facebook', 'GooglePlus', 'LinkedIn']
df_sm = df_test[col_sm]

df_sm = min_max_scaler.fit_transform(df_sm)
df_sm = pd.DataFrame(df_sm,columns =col_sm)
df_final_test = df_final_test.join(df_sm)

df_final_test.head()

#len(df_final_test.columns[df_final_test.isna().any()].tolist())

#df_final_test.isnull().sum().sum()/5754

json_file = open('model_c1d_8April_2259_t.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)

loaded_model.load_weights("model_c1d_8April_2259_t.h5")
print("Loaded model from disk")

loaded_model.compile(optimizer='Adam',
              loss='mse',
              metrics=['mae','accuracy'])

predict_t = loaded_model.predict(df_final_test,verbose=1)

predict_t

pd.DataFrame(predict_t).to_csv('predicted_t.csv')

p_t = pd.DataFrame(predict_t)

p_t.describe()




"""

